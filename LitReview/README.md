# Literature Review

## Table Format
| Paper Title | Description | Relevance to Our Research |
|-------------|------------|-------------------------|
| [**Alphaedit: Null-space constrained knowledge editing for language models**](https://arxiv.org/pdf/2410.02355) - [Code](https://github.com/jianghoucheng/AlphaEdit) | AlphaEdit is a model editing method that projects weight updates onto the null space of preserved knowledge, ensuring precise modifications without disrupting unrelated information. This approach enables controlled updates, such as factual corrections or bias mitigation, while maintaining overall model performance. | Apply AlphaEdit for debiasing by identifying where biased information resides in the model and correcting the space that needs to be adjusted. ( i.e Extract key-value pairs encoding the biased stereotype - Biased Subspace and then make neutral and unrelated data as preserved knowledge and edit the remaining to debias using Bias correction with counterfactuals? ) |
| [**Mass-editing memory in a transformer(MEMIT)**](https://arxiv.org/pdf/2210.07229) - [Code](https://memit.baulab.info) | MEMIT is a successor to previous work ROME, which performs a rank one modification of the MLP weights of a single layer to directly write a memory into the model.  | We will use MEMIT as our baseline while incorporating AlphaEdit’s null-space projection to enhance debiasing techniques |
| [**Debiasing Algorithm through Model Adaptation**](https://arxiv.org/pdf/2310.18913) - [Code](https://github.com/tomlimi/DAMA) | This paper causal analysis to identify problematic model components and uses Partial Least Squares algorithm ( PLS ) for identifying stereotype subspace and then removes it | This paper will be our main comparision as DAMA significantly decreases bias as measured by diverse metrics while maintaining the model’s performance on downstream tasks
| [**Potential and Challenges of Model Editing for Social Debiasing**](https://arxiv.org/pdf/2402.13462) - [Code ( Not public, we can mail authors )](https://github.com/ElliottYan/ModelEditingForDebias) | This paper propose two simple but effective methods to improve debias editing, it also formulates social debiasing into an editing problem  | We can check for single edit vs sequential editing performance in here, also it has some preprocessing before moving into editing for debiasing. It also talks about how editing can't generalize for unseen biases we can check on this data too.
| [**Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair Language Modeling and Translation**](https://arxiv.org/pdf/2501.10150) - [Analysis](https://www.aimodels.fyi/papers/arxiv/debiasing-algorithm-through-model-adaptation) | This paper proposes the preservation of factual gender cues by building on the base code from DAMA and applying the LEASE method instead of the PLS method for erasing biased signals. Dual DAMA has as an extra step to preserve knowledge, rather than simply erasing it biases. | This paper can be comparison for our work. Additionally, we can check and cite the bias mitigation results presented here to assess our approach.
| [**LoRA: Low-Rank Adaptation of Large Language Models**](https://arxiv.org/pdf/2106.09685) - [Code](https://github.com/microsoft/LoRA) | LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights. | As per DAMA paper LoRA manifests stronger debiasing properties, coming close to the results of DAMA in multiple bias metrics. But fine-tuning significantly deteriorates perplexity and the performance in language understanding tasks. This will be our baseline hence.
| [**ADEPT: A DEbiasing PrompT Framework**](https://arxiv.org/pdf/2211.05414) - [Code](https://github.com/EmpathYang/ADEPT) | This paper debias PLMs ( pre-trained language model) by prompt tuning with continuous prompts compared to discrete ones and ensure that the PLM does not lose its representation ability | We can have comparision with prompt tuned debiased model whether our editing technique works better than prompt tuning, also we can quote their mtrics on downstream tasks that they provided
| [**A Comprehensive Study of Knowledge Editing for Large Language Models**](https://arxiv.org/pdf/2401.01286) | This paper talks about techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs’ behaviors within specific domains while preserving overall performance across various inputs. | This paper helps in understanding about model editing can be quoted 
| [**Bias and Fairness in Large Language Models: A Survey**](https://arxiv.org/pdf/2309.00770) - [Code](https://github.com/i-gallegos/Fair-LLM-Benchmark) | This paper proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation | This paper helps in understanding current methodologies in bias mitigation ( our model editing can be compared with other processes? ) and also helps for checking on metrics for evaluation

This document will be regularly updated as new papers are reviewed.

