# Literature Review

## Table Format
| Paper Title | Description | Relevance to Our Research |
|-------------|------------|-------------------------|
| [**Alphaedit: Null-space constrained knowledge editing for language models**](https://arxiv.org/pdf/2410.02355) - [Code](https://github.com/jianghoucheng/AlphaEdit) | AlphaEdit is a model editing method that projects weight updates onto the null space of preserved knowledge, ensuring precise modifications without disrupting unrelated information. This approach enables controlled updates, such as factual corrections or bias mitigation, while maintaining overall model performance. | Apply AlphaEdit for debiasing by identifying where biased information resides in the model and correcting the space that needs to be adjusted. ( i.e Extract key-value pairs encoding the biased stereotype - Biased Subspace and then make neutral and unrelated data as preserved knowledge and edit the remaining to debias using Bias correction with counterfactuals? ) |
| [**Mass-editing memory in a transformer(MEMIT)**](https://arxiv.org/pdf/2210.07229) - [Code](https://memit.baulab.info) | MEMIT is a successor to previous work ROME, which performs a rank one modification of the MLP weights of a single layer to directly write a memory into the model.  | We will use MEMIT as our baseline while incorporating AlphaEditâ€™s null-space projection to enhance debiasing techniques |
| [**Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair Language Modeling and Translation**](https://arxiv.org/pdf/2501.10150) - [Analysis](https://www.aimodels.fyi/papers/arxiv/debiasing-algorithm-through-model-adaptation) | This paper proposes the preservation of factual gender cues by building on the base code from DAMA and applying the LEASE method instead of the PLS method for erasing biased signals. Dual DAMA has as an extra step to preserve knowledge, rather than simply erasing it biases. | This paper can be comparison for our work. Additionally, we can check and cite the bias mitigation results presented here to assess the effectiveness of our approach.
| [**Bias and Fairness in Large Language Models: A Survey**](https://arxiv.org/pdf/2309.00770) - [Code](https://github.com/i-gallegos/Fair-LLM-Benchmark) | This paper proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation | This paper helps in understanding current methodologies in bias mitigation ( our model editing can be compared with other processes? ) and also helps for checking on metrics for evaluation

This document will be regularly updated as new papers are reviewed.

